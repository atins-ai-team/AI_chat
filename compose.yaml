version: "3.9"

name: AI_chat

networks:
  ai_chat_network:
    driver: bridge

volumes:
  ollama:

services:
  init_runner:
    container_name: init_runner
    image: ollama/ollama
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    volumes:
      - ./ollama/entrypoint.sh:/entrypoint.sh
      - ollama:/root/.ollama  # jeśli potrzebne
    networks:
      - ai_chat_network
    restart: "no"  # <-- bardzo ważne
    depends_on: []
    env_file:
      - "models.env"

  ollama:
    container_name: ollama
    image: ollama/ollama
    networks:
      - ai_chat_network
    volumes:
      - ollama:/root/.ollama
    ports:
      - 11434:11434
    env_file:
      - "models.env"
    depends_on:
      init_runner:
        condition: service_completed_successfully
    tty: true
    restart: always
    deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: all
               capabilities:
                 - gpu

  ai_api:
    container_name: ai_api
    ports:
      - 8001:8001
    environment:
      - ollama_url=http://ollama:11434
    env_file:
      - "models.env"
    build:
      context: ./API
      dockerfile: Dockerfile
    networks:
      - ai_chat_network
    tty: true
    volumes:
      - ./API:/python-docker
    depends_on:
      - ollama

  frontend:
    build:
      context: ./Front
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    container_name: chatatins-front
    depends_on:
      - ai_api
